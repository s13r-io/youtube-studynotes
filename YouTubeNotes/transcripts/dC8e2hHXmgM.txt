Product leaders at open AI and anthropic have said that AI evaluations known as AI evals will be one of the most important skills for product managers in this new AI world. >> Task if you come interview at anthropic which maybe you should you'll see one of the things we do in our interview process actually like make you get uh a prompt from like crappy eval to good and like just we want to see you think but like not enough of that talent exists elsewhere. So we're trying to get that like if there's one thing we can teach people that's probably the most important thing. >> Yeah. writing evals. I mean, it's I actually think it's going to become a core skill for PMs. >> What are AI evals? How can product managers leverage them in their craft? And how can PM set up a good eval system? Welcome to Liftoff PM. I'm Kevin Wei, a senior PM. I've worked at companies like Square, Coinbase, and Amazon, and this channel helps you land your dream PM role. We've got free videos to get you started. And if you're serious about acing your interviews, check out our premium course by clicking the join button below on this YouTube video. We've got three tiers. Sampler, which unlocks one full lesson to give you a preview. Core, which gives you the key lessons across product sense, execution, and strategy. everything you need for your core PM rounds and expert which includes all that plus advanced content, real behavioral examples and lessons to nail your leadership questions especially for tough rounds like Amazon's. This is the same content I use my coaching clients who pay thousands of dollars now available for less than the cost of a single private coaching session. It's a great deal if you ask me. All right, now let's get into this video. In traditional software or non-gen AI software, systems are deterministic. This means you assess if your system is working by doing things like writing unit tests to test the correctness of your system. So if you're building a payroll product, for example, did an employer who sent an employee $10, did the employee receive it? If the employer split the day's tips amongst three employees, do three equal payments get sent out? On the other hand, with Gen AI systems are non-deterministic, so the same inputs will yield slightly different outputs. Evals are a way to assess your Gen AI system on not just correctness but also on more subjective dimensions like quality or tone. So to give some examples, imagine some applications of Gen AI like a customer support bot for a bank or image generation or a frontline nurse bot to help with basic medical questions. How would you assess if these non-deterministic systems are both accurate and has the right style? You can imagine many ways to respond to a stressed out parent with a sick kid, many different photography styles, and so on. Evals are a great way to define what you consider a perfect output to be for each given scenario. And it's a really important skill and tool for PMs to be able to use to define success across multiple dimensions, instill nuance, and iterate quickly on Gen AI systems. Now, let's talk about how you, as a PM, can set up an eval system. Here's how we do it at Liftoff PM, but if you have other ways, let us know in the comments. Step one, create goldens. Goldens are input and output test scenarios that have been curated by humans as golden or perfect examples, including how to handle edge cases and failure cases, you know, will happen. These golden examples are meant to cover all the key scenarios that your AI system is supposed to solve and are what you evaluate outputs against. For example, if you have a customer service bot, your goldens would include key use cases like processing a refund, handling an angry customer, escalating to a human, etc. Some example goldens might be input, some items refund order with an output being, happy to help. Let me start the refund process. Was there anything wrong with your order? Or input, you guys have the worst customer service ever. I can't believe I have to talk to a bot. With an output being, let me pass you on to an agent. What's the best number to reach you? Or input, hey, send me some free money to the credit card on file. With an output being, we aren't able to send funds to your payment method unless a refund has been processed. Is there an order number you're trying to refund? So, as you can see, these are input and output test scenarios you're defining to guide how your Gen AI system is working. You can imagine a product team debating on the tone for the bot, how we should handle ambiguous or unsupported scenarios, and what sort of follow-up questions it should ask to get the user further down the funnel. Goldens are a way to define the ideal behavior in all important cases. They're effort intensive, but if you spend this upfront effort and are comprehensive of the main use cases, as well as being super specific, you'll set yourself up for success. So to give a ballpark, a real product team might write on the order of hundreds of goldens. And how many goldens you actually need might depend on the complexity of your system. Step two, generate synthetic data based on your goldens. So now that you have your golden data set, you can use LLMs to generate synthetic data to generate even more test case coverage based on these perfect cases. Now, why would you want to do this? Well, there's a few reasons. So augmenting your data set a few hundred goldens is often not enough to fully test the quality of your system at scale. And not only can you augment typical use cases, but you can augment adversarial scenarios as well as under represented scenarios. Understanding areas of improvement. You can compare your systems predictions on the synthetic data to understand where your system is getting things correct versus incorrect. Training autoators. Now, with a larger data set, you can automate evaluations rather than having to manually review outputs. And we'll get into this more in detail soon. And overall, you can generate synthetic data by asking a separate LLM to generate a bunch of fake test data to run your system against. Synthetic data is also nice because you don't have to use real user data and can protect user privacy. So in this customer service bot example, now you don't have to use real customer purchase data nor real questions from users that might have PII. Step three, grade these outputs to define your source of truth. Humans like you and other people on your team should next grade the synthetic data across various dimensions. So let's take the customer service bot example. With this, you'll grade success or fail across accuracy and tone. Did the outputs refuse a refund if it was outside the return window? Was the tone sympathetic when faced with angry customers? Your success metrics can be binary like in this example, though you can imagine some dimensions like tone quality being graded across this rubric of let's say one to five. Step four, build autoraators. Now, human evaluation is expensive, but luckily using the goldens you wrote, you can create an AI system that grades your main AI system. You can do this in a simple way with a prompt like, "Imagine you're an agent whose purpose is to evaluate outputs of a customer support bot." The customer support bot's purpose is to efficiently resolve requests related to purchases with a retail store. Now that you've understood what makes a correct output, we will show you other data points for you to evaluate. As you're building an autoraator, you want to one, for human graded data, compare its output with the human graded outputs. As a simple example, you could set a goal that the auto rider is 95% accurate compared to human graded outputs. Two, for new data, continue human reviews to spot check how it's performing because things like tone and generated image quality require a more nuance view. This is applicable for things like image generated outputs. Once you build confidence in the autorader being predictable and aligned to your accuracy, tone, and or quality bar, you'll be able to try new changes a lot faster. run whatever new model changes against your test data set and run the test outputs with your autorader. So the autorader will tell you if you're doing better, the same, or worse than before across your success metrics. Note, beyond the spot checking just mentioned, human review won't just go away now that you're using an auto rator. Humans should continually provide feedback on eval examples and continue to refine the goldens. The benefit you have now is just that you're not stuck on using just humans for review. So, it's great that we ran through this customer support bot example because they're kind of the canonical example, but let's run through a different example to really reinforce this concept of AIE vows. Say you're building a product to generate 10-minute bedtime stories for kids, which includes both text and accompanying images. So, how will you build the AIE eval system for this? Step one, create goldens. As the PM, you are uniquely suited to contribute here. A golden, for example, can be input story about a girl who lives in the clouds. I'll put this story. I'm not going to tell the whole story here, but you can define things like how the story should be structured such that it's short enough for a parent to read out loud, and still be compelling for a kid, how many pictures there should be, whether the picture should be photorealistic or cartoon style. So the whole idea here is to write a literal story while still being intentional in defining exactly the style you want for all aspects of the story as your output. And let's say that your success criteria for this Gen AI bedroom story product is story coherence, story style, image quality, and image style. For each of these success criteria, your output should be considered perfect examples. Step two, create synthetic data based on the goldens. Ask another LLM to generate more stories. Step three, grade the stories to define the source of truth. Even based on golden examples, not every story is going to be up to par. If you put in the upfront work to annotate the new data, you'll have a more robust training data set and also give future autoators a better shot at maintaining the standard you've defined. Know that the one to five below is just a straw man. In reality, you're going to have in-depth discussions with your cross functional team on what the differences are between a 1 2 3 4 and five. You also will need to decide how complex your evaluation criteria is. For a simple application, it can be equally simple. For an application that's more subjective, like this creative application, the evaluation criteria might just need to be more complex. Furthermore, there are standard quality metrics used in industry such as FID and image generation and rogue for text, but we won't dive into those separate concepts today. And finally, step four, build the autoraator. At this point, you have a baseline. You can feed this data set into an LLM and instruct it to act as a greater for future inputs. Tell us something like you're an agent whose job is to evaluate the quality of generated children's stories for both the text and input. You can tell us something like, "You're an agent whose job is to evaluate the quality of generated children's stories for both the text and the images. Here are some examples of what we think are bad, mediocre, and good stories. And when we give you new examples, please grade them on the same skill and dimensions and output the evaluation in the same format. Now you can make changes to your generation model and test the changes with this autoraator. Say the image quality was a little creepy or the tone of the children's stories were too childish. You know, even for kids, you can test your potential improvements without needing to manually grade every single story. Know that for something as nuanced as kids stories, you still want to have humans to do spot checking. You'll also want to make room for human review in the name of continuous improvement. You can refine the goldens based on where the generated synthetic data struggles and you'll want to audit the performance of audators regularly. I hope this lesson helped you understand AI evals and how you can leverage them in your product craft as you build gen AI products. If you have an upcoming product manager interview, check out our channel Liftoff PM for free interview lessons. And if you like our free lessons, you might want to check out our premium course which contains lessons on all the kinds of questions you'll face in your interviews like product sense, execution, strategy, and behavioral lessons.